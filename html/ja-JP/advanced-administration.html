<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">第12章 高度な管理</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-ja-JP-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, 監視, 仮想化, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Debian 管理者ハンドブック" /><link
        rel="up"
        href="index.html"
        title="Debian 管理者ハンドブック" /><link
        rel="prev"
        href="sect.ldap-directory.html"
        title="11.7. LDAP ディレクトリ" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. 仮想化" /><link
        rel="canonical"
        href="http://l.github.io/debian-handbook/html/ja-JP/advanced-administration.html" /></head><body
      class="draft "><noscript><iframe
          src="//www.googletagmanager.com/ns.html?id=GTM-5H35QX"
          height="0"
          width="0"
          style="display:none;visibility:hidden"></iframe></noscript><script
        type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5H35QX');</script><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="http://debian-handbook.info"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.ldap-directory.html"><strong>戻る</strong></a></li><li
          class="home">Debian 管理者ハンドブック</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>次へ</strong></a></li></ul><div
        xml:lang="ja-JP"
        class="chapter"
        lang="ja-JP"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  id="advanced-administration"></a>第12章 高度な管理</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID と LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. ソフトウェア RAID</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID それとも LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. 仮想化</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#idm140338987810432">12.2.3. KVM を使った仮想化</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. 自動インストール</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Debian-Installer の preseed</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD、一体型の解決策</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. 監視</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Munin のセットアップ</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Nagios のセットアップ</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		この章では、われわれが既に説明した一部の側面を異なる視点から再度取り上げます。すなわち、1 台のコンピュータにインストールするのではなく、大規模な配備システムについて学びます。さらに、インストール時に RAID や LVM ボリュームを作成する代わりに、われわれは手作業でそれを行う方法について学びます。こうすることで最初の選択を訂正することが可能です。最後に、われわれは監視ツールと仮想化技術について議論します。その結果として、この章はより熟練した管理者を対象にしており、ホームネットワークに責任を負う個人を対象にしていません。
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    id="sect.raid-and-lvm"></a>12.1. RAID と LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">4章<em>インストール</em></a>では、インストーラの視点からこれらの技術を説明し、最初からこれらを簡単に配備するための統合方法について説明しました。最初のインストールの後、管理者は再インストールという手間のかかる最終手段を行使することなく、より大きなストレージ領域の要求に対処しなければいけません。管理者は RAID と LVM ボリュームを操作するために必要なツールを理解しなければいけません。
		</div><div
            class="para">
			RAID と LVM は両方ともマウントされたボリュームを物理的に対応する物 (実際のハードディスクドライブまたはそのパーティション) から抽象化する技術です。さらに、RAID は冗長性を導入することでデータをハードディスク障害から守り、LVM はボリューム管理をより柔軟にしてディスクの実サイズに依存しないようにします。どちらの場合であっても、最終的にシステムには新しいブロックデバイスが追加されますが、そのブロックデバイスをある物理ディスクに対応付けなくとも、ファイルシステムやスワップ領域を作成するために使うことが可能です。RAID と LVM は全く異なる生い立ちを持っていますが、両者の機能は多少重複しています。このため、両者は一緒に言及されることが多いです。
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPECTIVE</em></span> Btrfs が LVM と RAID を結び付ける</strong></p></div></div></div><div
              class="para">
			LVM と RAID は 2 種類の明らかに異なるカーネルサブシステムで、これはディスクブロックデバイスとそのファイルシステムの間のやり取りを担います。<span
                class="emphasis"><em>btrfs</em></span> は最初 Oracle で開発された新しいファイルシステムで、LVM と RAID の機能を結び付けると主張しています。<span
                class="emphasis"><em>btrfs</em></span> は開発がまだ完了していない (一部の機能がまだ実装されていない) ため「実験中」とタグ付けされていますが、大部分が機能し、本番環境で使われています。<div
                class="url">→ <a
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			<span
                class="emphasis"><em>btrfs</em></span> の特筆すべき機能に、任意の時点におけるファイルシステムツリーのスナップショットを取る機能があります。このスナップショットコピーは初期状態ではいかなるディスク領域も使いません、コピー内容の 1 つが修正された際にデータが複製されます。また、このファイルシステムはファイルを透過的に圧縮することが可能で、さらにチェックサムを用いて保存されているデータの完全性を保証します。
		</div></div><div
            class="para">
			RAID と LVM のどちらの場合も、カーネルはハードディスクドライブやパーティションに対応するものとよく似たブロックデバイスファイルを提供します。アプリケーションやカーネルの別の部分がそのようなデバイスのあるブロックにアクセスを要求する場合、適切なサブシステムがブロックを対応する物理層に転送します。設定に依存して、このブロックは 1 つか複数の物理ディスクに保存されます。その物理的場所は論理デバイス内のブロックの位置と直接的に対応するものではないかもしれません。
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.raid-soft"></a>12.1.1. ソフトウェア RAID</h3></div></div></div><a
              id="idm140339007895008"
              class="indexterm"></a><div
              class="para">
				RAID は <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> を意味します。このシステムの目標はハードディスク障害の際にデータ損失を防ぐことです。一般的な原則は極めて単純です。すなわち、データは設定できる冗長性のレベルに基づいて 1 つではなく複数のディスクに保存されます。冗長性の量に依存して、たとえ予想外のディスク障害が起きた場合でも、データを残りのディスクから損失なく再構成することが可能です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CULTURE</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">independent</em></span> それとも <span
                          class="foreignphrase"><em
                            class="foreignphrase">inexpensive</em></span>?</strong></p></div></div></div><div
                class="para">
				当初、RAID の I は <span
                  class="emphasis"><em>inexpensive</em></span> を意味していました。なぜなら、RAID は高価な高性能ディスクへ投資することなくデータの安全性を劇的に高めることが可能だったからです。しかしながらおそらく心証的な懸念から、現在 RAID の I は通例 <span
                  class="emphasis"><em>independent</em></span> を意味するものとされます。<span
                  class="emphasis"><em>independent</em></span> には安価であることに対する良くない印象がありません。
			</div></div><div
              class="para">
				RAID は専用ハードウェア (SCSI や SATA コントローラカードに統合された RAID モジュール) またはソフトウェア抽象化 (カーネル) を使って実装することが可能です。ハードウェアかソフトウェアかに関わらず、十分な冗長性を備えた RAID システムはディスク障害があっても透過的に利用できる状態を継続することが可能です。従って、スタックの上層 (アプリケーション) はディスク障害にも関わらず、引き続きデータにアクセスできます。もちろん「信頼性低下状態」は性能に影響を与え、冗長性を低下させます。このため、もう一つ別のディスク障害が起きるとデータを失うことになります。それ故、具体的に言えば管理者は障害の起きたディスクを交換して、せめて信頼性低下状態よりも状態を悪くしないように努力します。新しいディスクが配備されると、RAID システムは要求されたデータを再構成することが可能です。こうすることで信頼性の高い状態に戻ります。アプリケーションは、RAID アレイが信頼性低下状態か再構成状態の間にアクセス速度が低下する可能性のある点を除いて、何も気付かないでしょう。
			</div><div
              class="para">
				RAID がハードウェアで実装された場合、その設定は通常 BIOS セットアップツールによってなされます。カーネルは RAID アレイを標準的な物理ディスクとして機能する単一のディスクとみなします。RAID アレイのデバイス名は (ドライバに依存して) 違うかもしれません。
			</div><div
              class="para">
				本書ではソフトウェア RAID だけに注目します。
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.raid-levels"></a>12.1.1.1. さまざまな RAID レベル</h4></div></div></div><div
                class="para">
					実際のところ RAID は 1 種類だけというわけではなく、そのレベルによって識別される複数の種類があります。このため、設計と提供される冗長性の度合いが異なる複数の RAID レベルが存在します。より冗長性を高くすれば、より障害に強くなります。なぜなら、より多くのディスクで障害が起きても、システムを動かし続けることができるからです。これに応じて、与えられた一連のディスクに対して利用できる領域が小さくなります。つまり、あるサイズのデータを保存するために必要なディスク領域のサイズが多くなります。
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">リニア RAID</span></dt><dd><div
                      class="para">
								カーネルの RAID サブシステムを使うことで、「リニア RAID」を作ることも可能ですが、これは適切な RAID ではありません。なぜなら、このリニア RAID には冗長性が一切ないからです。カーネルはただ単純に複数のディスク端と端を統合し結果的に統合されたボリュームを仮想ディスク (1 つのブロックデバイス) として提供します。これがリニア RAID のすべてです。リニア RAID を使うのは極めてまれな場合に限られます (後から使用例を説明します)。特に、冗長性がないということは 1 つのディスクの障害が統合されたボリューム全体を駄目にする、そしてすべてのデータを駄目にすることを意味します。
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								RAID-0 レベルも冗長性を提供しませんが、順番通り単純に物理ディスクを連結する構成ではありません。すなわち、物理ディスクは<span
                        class="emphasis"><em>ストライプ状</em></span>に分割され、仮想デバイスのブロックは互い違いになった物理ディスクのストライプに保存されます。たとえば 2 台のディスクから構成されている RAID-0 セットアップでは、偶数を付番されたブロックは最初の物理ディスクに保存され、奇数を付番されたブロックは 2 番目の物理ディスクに保存されます。
							</div><div
                      class="para">
								RAID-0 システムは信頼性を向上させることではなく、性能を向上させることを目標にしています。システムの信頼性、データの可用性は (リニア RAID と同様に) ディスク障害があればすぐに脅かされるからです。つまり隣接した巨大なデータにシーケンシャルアクセスする場合、カーネルは両方のディスクから平行して読み込む (書き込む) ことが可能でしょう。このことにより、データの転送率が増加します。しかしながら、RAID-0 が使われる機会は減り、代わりに LVM (後から説明します) が使われるようになっています。
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								RAID-1 レベルは「RAID ミラーリング」としても知られ、最も簡単で最も広く使われています。RAID-1 の標準的な構造では、同じサイズの 2 台の物理ディスクを使い、物理ディスクと同じサイズの論理ボリュームが利用できるようになります。データを両方のディスクに保存するため、「ミラー」と呼ばれています。一方のディスクに障害があれば、他方のディスク上のデータを利用することが可能です。非常に重要なデータ用に、もちろん RAID-1 を 2 台以上の構成にすることも可能ですが、これはハードウェア費用と利用できる保存領域の比率に直接的な影響を与えます。
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> ディスクとクラスタサイズ</strong></p></div></div></div><div
                        class="para">
								異なるサイズの 2 台のディスクをミラーでセットアップする場合、サイズの大きい側のディスクは完全に利用されません。なぜなら、大きい側のディスクは最も小さいディスクと同じサイズのデータを含むに過ぎないからです。このため RAID-1 ボリュームで提供される利用できる領域のサイズは RAID アレイの最も小さなディスクのサイズと同じになります。冗長性を異なる方法で確保している、より高い RAID レベルの RAID ボリュームの場合に対しても同じことが言えます。
							</div><div
                        class="para">
								それ故、(RAID-0 と「リニア RAID」以外の) RAID アレイをセットアップする場合、資源の無駄を防ぐために、アレイを構成するディスクはそのサイズが完全に同じか近いものを使うことが重要です。
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> 予備ディスク</strong></p></div></div></div><div
                        class="para">
								冗長性を持たせた RAID レベルでは、アレイに必要なディスク数よりも多くのディスクを使うことが可能です。追加的ディスクは主要ディスクに障害が起きた場合に予備として使われます。たとえば、2 台のディスクと 1 台の予備ディスクのミラー構成では、最初の 2 台のうちの 1 台に障害が起きた場合、カーネルは自動的に (そして素早く) 予備ディスクを使ってミラーを再構成します。こうすることで、再構成の完了後に冗長性は確保されます。予備ディスクは、重要なデータに対する別の種類の安全装置として使うことが可能です。
							</div><div
                        class="para">
								この方式が単純に 3 台のディスクに対して最初からミラーリングを行うよりも優れているとされることに疑問を持つかもしれません。「予備ディスク」を設定する利点は複数の RAID ボリュームで予備ディスクを共有することが可能という点です。たとえば、1 台のディスク障害に対する冗長性を確保した 3 つのミラーされたボリュームを構成するには、ディスクを 7 台 (3 つのペアと 1 台の共有された予備) 用意するだけですみます。これに対して各ボリュームに 3 台のディスクを用意する場合には 9 台のディスクが必要です。
							</div></div><div
                      class="para">
								RAID-1 レベルは、高価であるにも関わらず (物理ストレージ領域の、良くても、たった半分しか使えない)、広く実運用されています。RAID-1 は簡単に理解でき、簡単にバックアップできます。なぜなら、両方のディスクが全く同じ内容を持っているため、運用システムに影響を与えることなしに、片方を一時的に取り外すことが可能だからです。通常、読み込み性能は上昇します。なぜなら、カーネルはデータの半分をそれぞれのディスクから平行して読むことができるからです。これに対して、書き込み性能はそれほど悪化しません。N 台のディスクからなる RAID-1 アレイの場合、データは N-1 台のディスク障害に対して保護されます。
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								RAID-4 は広く使われていません。RAID-4 は実データを保存するために N 台のディスクを使い、冗長性情報を保存するために 1 台の追加的ディスクを使います。追加的ディスクに障害が起きた場合、システムは他の N 台からデータを再構成することが可能です。N 台のデータディスクのうち、最大で 1 台に障害が起きた場合、残りの N-1 台と「パリティ」ディスクには、要求されたデータを再構成するために十分な情報が含まれます。
							</div><div
                      class="para">
								RAID-4 は高価過ぎるわけではありません。なぜなら、1 台当たりの費用は N 分の 1 だけ増加するに過ぎないからです。また、読み込み性能には大きな影響がありませんが、書き込み性能は悪化します。加えて、N 台の実データ用ディスクに対して書き込むと、パリティディスクに対する書き込みが発生するので、パリティディスクは実データ用ディスクに比べて書き込み回数が増えます。その結果、パリティディスクは極めて寿命が短くなります。RAID-4 アレイのデータは (N+1 台のディスクのうち) 1 台の障害に対して保護されます。
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 は RAID-4 の非対称性問題を対処したものです。パリティブロックは N+1 台のディスクに分散して保存され、特定のディスクが特定の役割を果たすことはありません。
							</div><div
                      class="para">
								読み込みと書き込み性能は RAID-4 と同様です。繰り返しになりますが、RAID-5 システムは (N+1 台のディスクのうち) 最大で 1 台までに障害が起きても動作します。
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 は RAID-5 の拡張と考えられます。RAID-6 では、N 個のブロックのそれぞれに対して 2 個の冗長性ブロックを使います。この N+2 個のブロックは N+2 台のディスクに分散して保存されます。
							</div><div
                      class="para">
								RAID-6 は RAID-4 と RAID-5 に比べて少し高価ですが、さらなる安全策を講じることが可能です。なぜなら、(N+2 台中の) 最大で 2 台までの障害に対してデータを守ることが可能だからです。書き込み操作は 1 つのデータブロックと 2 つの冗長性ブロックを書き込むことになりますから、書き込み性能がさらに遅くなります。
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								厳密に言えば RAID-1+0 は RAID レベルではなく、2 種類の RAID 分類を積み重ねたものです。2×N 台のディスクが必要で、最初に 2 台ずつのペアから N 台の RAID-1 ボリュームを作ります。N 台の RAID-1 ボリュームは「リニア RAID」か LVM (次第にこちらを選ぶケースが増えています) のどちらか一方を使って 1 台に統合されます。LVM を使うと純粋な RAID ではなくなりますが、LVM を使っても問題はありません。
							</div><div
                      class="para">
								RAID-1+0 は複数のディスク障害を乗り切ることが可能です。具体的に言えば、上に挙げた 2×N アレイの場合、最大で N 台までの障害に耐えます。ただし、各 RAID-1 ペアのうち、少なくとも 1 台のディスクが動き続けなければいけません。
							</div><div
                      class="sidebar"><a
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>GOING FURTHER</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								通常 RAID-10 は RAID-1+0 の同意語と考えられますが、Linux では特別に RAID-10 をより一般的な構成を可能にするものとして定めています。RAID-10 では、システムが各ブロックを 2 種類の異なるディスクに保存することが可能です。奇数台のディスク構成の場合でも、このコピーは設定可能なモデルに従って分散して保存されます。
							</div><div
                        class="para">
								性能は、選択した再分割モデルと冗長性の度合い、そして論理ボリュームの作業負荷に依存して変化します。
							</div></div></dd></dl></div><div
                class="para">
					明らかに、RAID レベルは各用途からの制限および要求に従って選択されます。1 台のコンピュータに、異なる設定を持つ複数の RAID アレイを配置することが可能である点に注意してください。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.raid-setup"></a>12.1.1.2. RAID の設定</h4></div></div></div><a
                id="idm140338995655024"
                class="indexterm"></a><div
                class="para">
					RAID ボリュームを設定するには <span
                  class="pkg pkg">mdadm</span> パッケージが必要です。さらに <span
                  class="pkg pkg">mdadm</span> パッケージには、RAID アレイを作成したり操作するための <code
                  class="command">mdadm</code> コマンド、システムの他の部分に RAID アレイを統合するためのスクリプトやツール、監視システム、が含まれます。
				</div><div
                class="para">
					以下の例では、多数のディスクを持つサーバをセットアップします。ディスクの一部は既に利用されており、残りは RAID をセットアップするために利用できるようになっています。最初の状態で、以下のディスクとパーティションが存在します。
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdb</code> ディスク、4 GB、は全部利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdc</code> ディスク、4 GB、も全部利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdd</code> ディスクでは、<code
                        class="filename">sdd2</code> (約 4 GB) パーティションだけが利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							最後に、<code
                        class="filename">sde</code> ディスク、4 GB、全部利用できます。
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> 既存の RAID ボリュームの識別</strong></p></div></div></div><div
                  class="para">
					<code
                    class="filename">/proc/mdstat</code> ファイルには、既存のボリュームとその状態が書かれています。新しい RAID ボリュームを作成する場合、既存のボリュームと同じ名前を付けないように注意してください。
				</div></div><div
                class="para">
					2 つのボリューム、RAID-0 とミラー (RAID-1)、を作るためにこれらの物理ディスクを使います。RAID-0 ボリュームから始めましょう。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">ファイルシス   サイズ  使用  残り 使用% マウント位置
/dev/md0         7.9G   18M  7.4G    1% /srv/raid-0
</code></pre><div
                class="para">
					<code
                  class="command">mdadm --create</code> コマンドは複数のパラメータを要求します。具体的に言えば、作成するボリュームの名前 (<code
                  class="filename">/dev/md*</code>、MD は <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span> を意味します)、RAID レベル、ディスク数 (普通この値は RAID-1 とそれ以上のレベルでのみ意味があるにも関わらず、これは必須オプションです)、利用する物理デバイスを要求します。デバイスを作成したら、デバイスを通常のパーティションを取り扱うのと同様のやり方で取り扱うことが可能です。ファイルシステムを作成したり、ファイルシステムをマウントしたり、することが可能です。作成する RAID-0 ボリュームに <code
                  class="filename">md0</code> と名前を付けたのは偶然に過ぎない点に注意してください。アレイに付けられた番号と冗長性の度合いを関連付ける必要はありません。また、<code
                  class="filename">/dev/md0</code> の代わりに <code
                  class="filename">/dev/md/linear</code> のような <code
                  class="command">mdadm</code> パラメータを使えば、名前付き RAID アレイを作成することも可能です。
				</div><div
                class="para">
					同様のやり方を使って RAID-1 を作成します。注意するべき違いは作成後に説明します。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> RAID、ディスク、パーティション</strong></p></div></div></div><div
                  class="para">
					上の例で示した通り、RAID デバイスはディスクパーティションに作成することが可能です。必ずディスク全体を使わなければいけないというわけではありません。
				</div></div><div
                class="para">
					いくつかの注意点があります。最初に、<code
                  class="command">mdadm</code> は物理デバイス同士のサイズが異なる点を指摘しています。さらに、このことによりサイズが大きい側のデバイスの一部の領域が使えなくなるため、確認が求められています。
				</div><div
                class="para">
					さらに重要なことは、ミラーの状態に注意することです。RAID ミラーの正常な状態とは、両方のディスクが全く同じ内容を持っている状態です。しかしながら、ボリュームを最初に作成した直後、RAID ミラーは正常な状態であることを保証されません。このため、RAID サブシステムは RAID ミラーの正常な状態を保証するために、RAID デバイスが作成されたらすぐに同期化作業を始めます。しばらくの後 (必要な時間はディスクの実サイズに依存します…)、RAID アレイは「active」または「clean」状態に移行します。同期化作業中、ミラーは信頼性低下状態で、冗長性は保証されない点に注意してください。同期化作業中にディスク障害が起きると、すべてのデータを失うことにつながる恐れがあります。しかしながら、最近作成された RAID アレイの最初の同期化作業の前に、大量の重要なデータがこの RAID アレイに保存されていることはほとんどないでしょう。信頼性低下状態であっても、<code
                  class="filename">/dev/md1</code> を利用することが可能で、ファイルシステムを作成したり、データのコピーを取ったりすることが可能、という点に注意してください。
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> 信頼性低下状態でミラーを開始する</strong></p></div></div></div><div
                  class="para">
					RAID-1 ミラーを構成する 2 台のディスクの両方をすぐに使えないことが時々あります。たとえば、ミラーを構成するディスクの片方に、ミラーに移動したいデータが既に保存されている場合です。このような場合、<code
                    class="command">mdadm</code> に渡すデバイスファイル引数の片方をデバイスファイルの代わりに <code
                    class="filename">missing</code> にすることで、意図的に信頼性低下状態の RAID-1 アレイを作成することも可能です。ミラーに移動したいデータを含むディスクからデータを「ミラー」にコピーした後、そのディスクをアレイに追加することが可能です。追加作業が終われば、同期化作業が行われ、ミラーに移動したかったデータの冗長性が確保されます。
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> 同期化作業を行わずにミラーをセットアップする</strong></p></div></div></div><div
                  class="para">
					RAID-1 ボリュームは通常新しいディスクとして使うために作成され、そのディスクにはデータが保存されていないと考えられます。このため、RAID を構成するディスクの初期内容は特に意味を持ちません。ボリュームの作成前、具体的に言えば初期ファイルシステムの作成前、に書き込まれていたデータは作成後にアクセスできないという点に注意してください。
				</div><div
                  class="para">
					そんなわけで、作成された時点で両方のディスクの同期化が行われる点について疑問に思うかもしれません。内容はゾーンに書き込まれた後で読み込まれるにも関わらず、なぜ、ボリュームのゾーン上で内容が同一であるか否かに注意する必要があるのでしょうか?
				</div><div
                  class="para">
					幸いなことに、同期化作業は <code
                    class="literal">--assume-clean</code> オプションを <code
                    class="command">mdadm</code> に渡せば避けることが可能です。しかしながら、初期データが読まれる場合、このオプションを使うと問題があります (たとえば、物理ディスク上にファイルシステムが既に存在している場合、問題があります)。このため、デフォルトでこのオプションは有効化されません。
				</div></div><div
                class="para">
					RAID-1 アレイを構成するディスクの 1 台に障害が発生した場合、何が起きるかを見て行きましょう。<code
                  class="command">mdadm</code> に <code
                  class="literal">--fail</code> オプションを付けることで、ディスク障害を模倣することが可能です。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					ボリュームの内容はまだアクセスすることが可能 (そして、ボリュームがマウントされていた場合、アプリケーションはディスク障害に気が付きません) ですが、データの安全性はもはや保証されません。つまり <code
                  class="filename">sdd</code> ディスクに障害が発生した場合、データは失われます。この危険性を避けるために、われわれは障害の発生したディスクを新しいディスク <code
                  class="filename">sdf</code> に交換します。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					繰り返しになりますが、ボリュームはまだアクセスすることが可能ですが、ボリュームが信頼性低下状態ならば、カーネルは自動的に再構成作業を実行します。再構成作業が終了したら、RAID アレイは正常状態に戻ります。ここで、システムに <code
                  class="filename">sde</code> ディスクをアレイから削除することを伝えることが可能です。削除することで、2 台のディスクからなる古典的な RAID ミラーになります。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					この後、今後サーバの電源を切った後にドライブを物理的に取り外したり、ハードウェア設定がホットスワップに対応しているならばホットリムーブすることが可能です。一部の SCSI コントローラ、ほとんどの SATA ディスク、USB や Firewire で接続された外部ドライブなどはホットスワップに対応しています。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.backup-raid-config"></a>12.1.1.3. 設定のバックアップ</h4></div></div></div><div
                class="para">
					RAID ボリュームに関連するメタデータのほとんどはアレイを構成するディスク上に直接保存されています。このため、カーネルはアレイとその構成要素を検出し、システムの起動時に自動的にアレイを組み立てることが可能です。しかしながら、この設定をバックアップすることを推奨します。なぜなら、この検出機構は不注意による間違いを防ぐものではないからです。そして、注意して取り扱うべき状況ではまさに検出機構がうまく働かないことが見込まれます。上の例で、<code
                  class="filename">sde</code> ディスク障害が本物で (模倣でない)、<code
                  class="filename">sde</code> ディスクを取り外す前にシステムを再起動した場合、<code
                  class="filename">sde</code> ディスクは再起動中に検出され、システムに復帰します。カーネルは 3 つの物理ディスクを検出し、それぞれのディスクが同じ RAID ボリュームの片割れであると主張します。さらに別の混乱する状況が考えられます。2 台のサーバで使われていた RAID ボリュームを片方のサーバに集約することを考えてみましょう。ディスクが移動される前、各アレイは正常に実行されていました。カーネルはアレイを検出して、適切なペアを組み立てることが可能です。しかし、片方のサーバに移動されたディスクが前のサーバでは <code
                  class="filename">md1</code> に組み込まれており、さらに新しいサーバが既に <code
                  class="filename">md1</code> という名前のアレイを持っていた場合、どちらか一方の名前が変えられます。
				</div><div
                class="para">
					このため、参考情報に過ぎないとは言うものの、設定を保存することは重要です。設定を保存する標準的な方法は <code
                  class="filename">/etc/mdadm/mdadm.conf</code> ファイルを編集することです。以下に例を示します。
				</div><div
                class="example"><a
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>例12.1 <code
                      class="command">mdadm</code> 設定ファイル</strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</pre></div></div><div
                class="para">
					最も役に立つ設定項目の 1 つに <code
                  class="literal">DEVICE</code> オプションがあります。これは起動時にシステムが RAID ボリュームの構成情報を自動的に探すデバイスをリストします。上の例では、デフォルト値 <code
                  class="literal">partitions containers</code> を置き替え、デバイスファイルを明示したリストにしました。なぜなら、パーティションだけでなくすべてのディスクをボリュームとして使うように決めたからです。
				</div><div
                class="para">
					上の例における最後の 2 行を使うことで、カーネルはアレイに割り当てるボリューム番号を安全に選ぶことが可能です。ディスク本体に保存されたメタ情報はボリュームを再度組み上げるのに十分ですが、ボリューム番号を定義する (そして <code
                  class="filename">/dev/md*</code> デバイス名にマッチすることを確認する) には不十分です。
				</div><div
                class="para">
					幸いなことに、この行を自動的に生成することが可能です。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					最後の 2 行の内容はボリュームを構成するディスクのリストに依存しません。このため、障害の発生したディスクを新しいディスクに交換した際に、これを改めて生成する必要はありません。逆に、RAID アレイを作成および削除した際に、必ずこの設定ファイルを注意深く更新する必要があります。
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="idm140338987011504"
              class="indexterm"></a><a
              id="idm140338987010544"
              class="indexterm"></a><div
              class="para">
				LVM、<span
                class="emphasis"><em>論理ボリュームマネージャ</em></span>、は物理ディスクから論理ボリュームを抽象化する別のアプローチで、信頼性を増加させるのではなく柔軟性を増加させることに注目しています。LVM を使うことで、アプリケーションから見る限り透過的に論理ボリュームを変更することが可能です。さらに、たとえば LVM は、新しいディスクを追加し、データを新しいディスクに移行し、古いディスクを削除することが、ボリュームをアンマウントせずに可能です。
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-concepts"></a>12.1.2.1. LVM の概念</h4></div></div></div><div
                class="para">
					LVM の柔軟性は 3 つの概念から成る、抽象化レベルによって達成されます。
				</div><div
                class="para">
					最初に、PV (<span
                  class="emphasis"><em>物理ボリューム</em></span>) はハードウェアに最も近い物です。具体的に言えば、PV はディスクのパーティション、ディスク全体、その他の任意のブロックデバイス (たとえば、RAID アレイ) などの物理的要素です。物理的要素を LVM の PV に設定した場合、物理的要素へのアクセスは必ず LVM を介すべきという点に注意してください。そうでなければ、システムが混乱します。
				</div><div
                class="para">
					複数の PV は VG (<span
                  class="emphasis"><em>ボリュームグループ</em></span>) にクラスタ化することが可能です。VG は仮想的ディスクや拡張できるディスクと比較されます。VG は概念的なもので、<code
                  class="filename">/dev</code> 階層のデバイスファイルに現れません。そのため、VG を直接的に操作する危険はありません。
				</div><div
                class="para">
					3 番目のオブジェクトは LV (<span
                  class="emphasis"><em>論理ボリューム</em></span>) です。LV は VG の塊です。さらに VG をディスクと比較する類推に従って考えると、LV はパーティションと比較されます。LV はブロックデバイスとして <code
                  class="filename">/dev</code> に現れ、他の物理パーティションと同様に取り扱うことが可能です (一般的に言って、ファイルシステムやスワップ領域を作成することが可能です)。
				</div><div
                class="para">
					重要な事柄は、VG を LV に分割する場合に物理的要素 (PV) はいかなる制約も要求しないという点です。1 つの物理的要素 (たとえばディスク) から構成される VG は複数の論理ボリュームに分割できます。従って同様に、VG に複数の物理ディスクを参加させ、VG を 1 つの大きな論理ボリュームとして提供することが可能です。明らかに、たった 1 つの制約事項は各 LV に割り当てるサイズの合計が、LV が所属するボリュームグループの PV の合計サイズを超えることができないという点です。
				</div><div
                class="para">
					しかしながら、VG を構成する物理的要素の特徴を一致させること、各論理ボリュームの用途に対する要求性能を考慮しながら VG を論理ボリュームに分割すること、は通常理に適った方針です。たとえば、利用できるハードウェアに高速なディスクと低速なディスクがある場合、高速なディスクから構成される VG と低速なディスクから構成される VG に分けることが可能です。このため、高速なディスクから構成される VG に含まれる論理ボリュームを高速なデータアクセスを必要とするアプリケーションに割り当て、低速なディスクから構成される VG に含まれる論理ボリュームを負荷の少ない作業用に割り当てます。
				</div><div
                class="para">
					いかなる場合でも、LV は特定の PV に所属しないということを覚えておいてください。ある LV に含まれるデータが物理的に保存されている場所を操作することも可能ですが、普通に使っている限りその必要はありません。逆に、VG を構成する一連の物理的要素が変化する場合、特定の LV に対応する物理ストレージの場所をディスク上で変化させることが可能です (もちろん、変化の範囲は対象の VG を構成する PV の中に限られます)。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-setup"></a>12.1.2.2. LVM の設定</h4></div></div></div><div
                class="para">
					典型的な用途に対する LVM の設定過程を、段階的に見て行きましょう。具体的に言えば、複雑なストレージの状況を単純化したい場合を見ていきます。通常、長く複雑な一時的措置を繰り返した挙句の果てに、この状況に陥ることがあります。説明目的で、徐々にストレージを変更する必要のあったサーバを考えます。このサーバでは、複数の一部使用済みディスクに、利用できるパーティションが分けられています。より具体的に言えば、以下のパーティションが利用できます。
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdb</code> ディスク上に、<code
                        class="filename">sdb2</code> パーティション、4 GB。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdc</code> ディスク上に、<code
                        class="filename">sdc3</code> パーティション、3 GB。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdd</code> ディスク、4 GB、は全領域を利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdf</code> ディスク上に、<code
                        class="filename">sdf1</code> パーティション、4 GB。および <code
                        class="filename">sdf2</code> パーティション、5 GB。
						</div></li></ul></div><div
                class="para">
					加えて、<code
                  class="filename">sdb</code> と <code
                  class="filename">sdf</code> が他の 2 台に比べて高速であると仮定しましょう。
				</div><div
                class="para">
					われわれの目標は、3 種類の異なるアプリケーション用に 3 つの論理ボリュームを設定することです。具体的に言えば、5 GB のストレージ領域が必要なファイルサーバ、データベース (1 GB)、バックアップ用の領域 (12 GB) を設定することです。ファイルサーバとデータベースは高い性能を必要とします。しかし、バックアップはアクセス速度をそれほど重要視しません。これらすべての制約事項により、各アプリケーションの使うパーティションが制限されます。さらに LVM を使うことで、デバイスの物理的サイズをまとめることが可能です。このため、利用できる領域の合計だけが制限です。
				</div><div
                class="para">
					必要なツールは <span
                  class="pkg pkg">lvm2</span> パッケージとその依存パッケージに含まれています。これらのパッケージをインストールしたら、3 つの手順を踏んで LVM をセットアップします。各手順は LVM の概念の 3 つのレベルに対応します。
				</div><div
                class="para">
					最初に、<code
                  class="command">pvcreate</code> を使って物理ボリュームを作成します。
				</div><a
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					ここまでは順調です。さらに PV はディスク全体およびディスク上の各パーティションに対して設定することが可能です。上に示した通り、<code
                  class="command">pvdisplay</code> コマンドは既存の PV をリストします。出力フォーマットは 2 種類あります。
				</div><div
                class="para">
					<code
                  class="command">vgcreate</code> を使って、これらの物理的要素から VG を構成しましょう。高速なディスクの PV から <code
                  class="filename">vg_critical</code> VG を構成します。さらに、これ以外の低速なディスクを含む PV から <code
                  class="filename">vg_normal</code> VG を構成します。
				</div><a
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					繰り返しになりますが、コマンドはかなり簡潔です (<code
                  class="command">vgdisplay</code> には 2 種類の出力フォーマットがあります)。同じ物理ディスクの 2 つのパーティションを 2 つの異なる VG に割り当てることが可能である点に注意してください。また、<code
                  class="filename">vg_</code> 接頭辞を VG の名前に使っていますが、これは慣例に過ぎない点に注意してください。
				</div><div
                class="para">
					これでサイズが約 8 GB と 12 GB の 2 台の「仮想ディスク」を手に入れたことになります。それでは仮想ディスクを「仮想パーティション」(LV) に分割しましょう。これを行うには <code
                  class="command">lvcreate</code> コマンドを少し複雑な構文で実行します。
				</div><a
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					論理ボリュームを作成する場合、2 種類のパラメータが必要です。このため、必ず 2 種類のパラメータをオプションとして <code
                  class="command">lvcreate</code> に渡します。作成する LV の名前を <code
                  class="literal">-n</code> オプションで指定し、サイズを <code
                  class="literal">-L</code> オプションで指定します。また、操作対象の VG をコマンドに伝えることが必要です。これはもちろん最後のコマンドラインパラメータです。
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>GOING FURTHER</em></span> <code
                            class="command">lvcreate</code> オプション</strong></p></div></div></div><div
                  class="para">
					<code
                    class="command">lvcreate</code> コマンドは複数のオプションを取り、作成する LV を微調整することが可能です。
				</div><div
                  class="para">
					最初に <code
                    class="literal">-l</code> オプションについて説明しましょう。<code
                    class="literal">-l</code> オプションを使った場合 LV のサイズをブロック数で指定することが可能です (上の例で用いた「人間にとって分かりやすい」単位ではありません)。ブロックとは (LVM の用語で PE、<span
                    class="emphasis"><em>物理エクステント</em></span>、と呼ばれています) PV 中のストレージ領域の連続した単位です。ブロックは LV 中に分散されています。正確にある LV 用のストレージ領域を定義したい場合、たとえば利用できる領域のすべてを使いたい場合、<code
                    class="literal">-l</code> オプションのほうが <code
                    class="literal">-L</code> オプションよりも使いやすいでしょう。
				</div><div
                  class="para">
					LV の物理的な位置を指定することも可能です。こうすることで、LV のエクステントが特定の PV 上 (もちろん、VG を構成する PV 上に限ります) に作られます。<code
                    class="filename">sdb</code> が <code
                    class="filename">sdf</code> よりも高速なので、<code
                    class="filename">lv_base</code> を <code
                    class="filename">sdb</code> 上に作成することで、ファイルサーバよりもデータベースサーバが高速にアクセスできるようにしたいとわれわれは考えています。コマンドラインは次のようになります。すなわち <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code> です。指定した PV に十分な空きエクステントがない場合、このコマンドは失敗する可能性があります。われわれの例で、このような失敗を防ぐには <code
                    class="filename">lv_files</code> の前に <code
                    class="filename">lv_base</code> を作成するか、<code
                    class="command">pvmove</code> コマンドを使って <code
                    class="filename">sdb2</code> に多少の領域を空ける必要があるかもしれません。
				</div></div><div
                class="para">
					論理ボリュームが作成され、ブロックデバイスファイルとして <code
                  class="filename">/dev/mapper/</code> に現れます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">合計 0
crw------- 1 root root 10, 236  6月 10 16:52 control
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0  6月 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1  6月 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2  6月 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> LVM ボリュームの自動検出</strong></p></div></div></div><div
                  class="para">
					コンピュータの起動時に、<code
                    class="filename">lvm2-activation</code> systemd サービスユニットは <code
                    class="command">vgchange -aay</code> を実行してボリュームグループを「始動」します。つまり、<code
                    class="filename">lvm2-activation</code> systemd サービスユニットは利用できるデバイスを探します。そして LVM 用の物理ボリュームとして初期化されたデバイスは LVM サブシステムに登録され、ボリュームグループに所属するデバイスは組み上げられ、対応する論理ボリュームは開始され、利用できるようにされます。このため、LVM ボリュームを作成したり変更する際に設定ファイルを編集する必要はありません。
				</div><div
                  class="para">
					しかしながら、LVM 要素の配置図 (物理ボリューム、論理ボリューム、ボリュームグループ) は <code
                    class="filename">/etc/lvm/backup</code> にバックアップされ、問題が起きた時 (見えないところで何が行われているかを確認したい時) に有益です。
				</div></div><div
                class="para">
					分かり易くするために、VG に対応するディレクトリの中に便利なシンボリックリンクが作成されます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					LV は標準的なパーティションと全く同様に取り扱われます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_normal-lv_backups    12G   30M   12G    1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					アプリケーションにしてみれば、無数の小さなパーティションがわかり易い名前を持つ 1 つの大きな 12 GB のボリュームにまとめられたことになります。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM の経時変化</h4></div></div></div><div
                class="para">
					パーティションや物理ディスクを統合する機能は便利ですが、これは LVM のもたらす主たる利点ではありません。ボリュームを進化させる必要が生じた場合に、LVM のもたらす柔軟性が時間経過に伴い特に重要視されるようになります。われわれの例で、新たに巨大なファイルを保存したいけれども、ファイルサーバ用の LV はこの巨大なファイルを保存するには狭すぎる、と仮定しましょう。われわれはまだ <code
                  class="filename">vg_critical</code> で利用できる全領域を使い切っていないので、<code
                  class="filename">lv_files</code> のサイズを増やすことが可能です。この目的のために <code
                  class="command">lvresize</code> コマンドを使い、ボリュームのサイズの変化にファイルシステムを対応させるために <code
                  class="command">resize2fs</code> を使います。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   5.0G  4.6G  146M   97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   6.9G  4.6G  2.1G   70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>CAUTION</em></span> ファイルシステムのサイズ変更</strong></p></div></div></div><div
                  class="para">
					すべてのファイルシステムがオンラインでサイズを変更できるわけではありません。このため、ボリュームサイズを変更するには、最初にファイルシステムのアンマウントし、その後に再マウントしなければいけません。もちろん、LV に割り当てられた領域サイズを小さくしたい場合、最初にファイルシステムのサイズを小さくしなければいけません。サイズを大きくする場合、この順番は逆転します。すなわち、ファイルシステムの前に論理ボリュームのサイズを大きくされなければいけません。これはかなりわかり易いです。なぜなら、ブロックデバイス上に存在するファイルシステムのサイズをブロックデバイスよりも大きくすることは絶対に不可能だからです (この原則はデバイスが物理パーティションか論理ボリュームかに依存しません)。
				</div><div
                  class="para">
					ext3、ext4、xfs ファイルシステムはオンラインでサイズを増加させることが可能で、アンマウントが不要です。しかし、サイズを減少させる場合はアンマウントを必要とします。reiserfs はオンラインでサイズを増加および減少することが可能です。ext2 は増加も減少も可能ですが、アンマウントが必要です。
				</div></div><div
                class="para">
					データベースをホストしているボリュームを拡張するために、同じやり方を使います。VG の利用できる領域は既にほぼ使い切った状態になっています。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base  1008M  854M  104M   90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					どんな状況であっても、LVM を使っていれば物理ボリュームを既存のボリュームグループに追加することが可能です。たとえば、今までは LVM の外で管理されていた <code
                  class="filename">sdb1</code> パーティションには、<code
                  class="filename">lv_backups</code> に移動しても問題のないアーカイブだけが含まれていた点に気が付いたとしましょう。このため、<code
                  class="filename">sdb1</code> パーティションを再利用し、ボリュームグループに統合させることが可能です。その結果、利用できる領域を増やすことが可能です。これが <code
                  class="command">vgextend</code> コマンドの目的です。もちろん、<code
                  class="filename">sdb1</code> パーティションを物理ボリュームとして準備しなければいけません。VG を拡張したら、先と同様のコマンドを使って先に論理ボリュームを増加させ、その後にファイルシステムを増加させます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base   2.0G  854M  1.1G   45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>GOING FURTHER</em></span> 上級の LVM</strong></p></div></div></div><div
                  class="para">
					また、LVM にはさらに上級の使い方があり、多くの設定事項を手作業で指定することが可能です。たとえば、管理者は物理および論理ボリュームを形作るブロックサイズおよびボリュームの物理的な配置を微調整することが可能です。また、ブロックを PV 間で移動することも可能です。これは、たとえば性能を微調整したり、よりありふれたケースでは (別の VG に移動したり、完全に LVM から取り外すために) 対応する物理ボリュームを VG から取り外したい PV を空にする目的で行われます。コマンドを説明しているマニュアルページは基本的に明快で詳細です。手始めに、<span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span> マニュアルページを参照することをお勧めします。
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.raid-or-lvm"></a>12.1.3. RAID それとも LVM?</h3></div></div></div><div
              class="para">
				RAID と LVM はどちらも、用途が変わらない 1 台のハードディスクを備えたデスクトップコンピュータの単純なケースではすぐに、疑う余地のない利点をもたらします。しかしながら、RAID と LVM は目標を分岐させて別々の道を歩んでいます。どちらを使うべきか悩むのは間違っていることではありません。最も適切な答えは、もちろん現在の要求と将来に予測される要求に依存します。
			</div><div
              class="para">
				いくつかの状況では、疑問の余地がないくらい簡単に答えを出すことが可能です。ハードウェア障害からデータを保護することが求められる場合、ディスクの冗長性アレイ上に RAID をセットアップするのは明らかです。なぜなら LVM はこの種の問題に全く対処しないからです。逆に、柔軟なストレージ計画が必要でディスクの物理的な配置に依存せずにボリュームを構成したい場合、RAID はあまり役に立たず LVM を選ぶのが自然です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> 性能を重要視する場合…</strong></p></div></div></div><div
                class="para">
				入出力速度が最重要の場合、特にアクセス速度という意味で、LVM および RAID を組み合わせて使うと性能に少なからず影響を与え、どの方式を採用するか関する議論に影響を与えます。しかしながら、性能差は極めて少なく、この程度の性能差が無視できない場合は極めて少ないと言えるでしょう。性能が重要な場合、実現できる最良の改善方針は非回転ストレージメディア (<a
                  id="idm140338986892640"
                  class="indexterm"></a><span
                  class="emphasis"><em>ソリッドステートドライブ</em></span>や SSD) を使うことです。SSD のメガバイト当たりの費用は標準的なハードディスクドライブよりも高価で、SSD の容量は通常小さいですが、SSD はランダムアクセスで素晴らしい性能を発揮します。ファイルシステムに広く分散された位置から数多くの入出力を行うような場合、たとえば複雑な問い合わせが定期的に実行されるデータベースの場合、SSD 上にデータベースを置くことは、RAID over LVM または LVM over RAID 上にデータベースを置くことよりも、良好な性能が手に入ります。このような場合、純粋な速度だけでなく他の要素も検討した上で採用の可否を決定するべきです。なぜなら、性能が必要な場合、SSD を採用することが最も短絡的な解決策だからです。
			</div></div><div
              class="para">
				3 番目の注目すべき利用形態は単に 2 つのディスクを 1 つのボリュームにまとめる場合です。これは性能が欲しかったり、利用できるディスクのどれよりも大きな単一のファイルシステムにするという理由が考えられます。この場合、RAID-0 (またはリニア RAID) か LVM ボリュームを使って対処できます。この状況では、追加的な制約事項 (たとえば、残りのコンピュータが RAID だけを使っている場合に RAID を使わなければいけないなど) がなければ、通常 LVM を選択します。LVM の最初のセットアップは RAID に比べて複雑ですが、LVM は複雑度を少し増加させるだけで、要求が変った場合や新しいディスクを追加する必要ができた場合に対処可能な、追加的な柔軟性を大きく上昇させます。
			</div><div
              class="para">
				そしてもちろん、本当に興味深い利用形態はストレージシステムにハードウェア障害に対する耐性を持たせさらにボリューム割り当てに対する柔軟性を持たせる必要がある場合です。RAID と LVM のどちらも片方だけで両方の要求を満足させることは不可能です。つまりこの要求を満足させるには、RAID と LVM の両方を同時に使用するというよりも、一方の上に他方を構成する方針を採用すべきです。RAID と LVM は成熟しているため、この方針はほぼ標準的になりつつあります。この方針では、最初にディスクを少数の大きな RAID アレイにグループ分けして、それらの RAID アレイを LVM 物理ボリュームとして使うことにより、データの冗長性を確保します。そして論理パーティションはファイルシステム用の LV から分配されます。この方針の売りは、ディスク障害が起きた場合に再構築しなければいけない RAID アレイの数が少ない点です。このため、管理者は復旧に必要な時間を減らすことが可能です。
			</div><div
              class="para">
				ここで具体例を見てみましょう。たとえば Falcot Corp の広報課では、動画編集用にワークステーションが必要ですが、広報課の予算の都合上、最初から高性能のハードウェアに投資することは不可能です。グラフィック性能を担うハードウェア (モニタとビデオカード) に大きな予算を割き、ストレージ用には一般的なハードウェアを使うように決定されました。しかしながら、広く知られている通り、デジタルビデオ用のストレージはある種の条件を必要とします。つまり、保存されるデータのサイズが大きく、このデータを読み込みおよび書き込みする際の処理速度がシステム全体の性能にとって重要 (たとえば、平均的なアクセス時間よりも重要) です。この条件を一般的なハードウェアを使って満足させます。この場合 2 台の SATA ハードディスクドライブを使います。さらに、システムデータと一部のユーザデータはハードウェア障害に対する耐性を持たせる必要があります。編集済みのビデオクリップを保護しなければいけませんが、編集前のビデオ素材をそれほど気にする必要はありません。なぜなら、編集前のビデオ素材はまだビデオテープで残されているからです。
			</div><div
              class="para">
				前述の条件を満足させるために RAID-1 と LVM を組み合わせます。ディスクの並行アクセスを最適化し、そして障害が同時に発生する危険性を減らすために、各ディスクは 2 つの異なる SATA コントローラに接続されています。このため、各ディスクは <code
                class="filename">sda</code> と <code
                class="filename">sdc</code> として現れます。以下の方針で示す通り、全く同一の方法でパーティショニングされます。
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						両方のディスクの最初のパーティション (約 1 GB) は RAID-1 ボリューム <code
                      class="filename">md0</code> に組み込まれます。このミラーは root ファイルシステムを保存するために直接的に使われます。
					</div></li><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda2</code> と <code
                      class="filename">sdc2</code> パーティションは swap パーティションとして使われます。スワップ領域のサイズは合計で 2 GB になります。RAM のサイズは 1 GB で、ワークステーションで利用できるメモリサイズは十分な量と言えます。
					</div></li><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda5</code> と <code
                      class="filename">sdc5</code> パーティションおよび <code
                      class="filename">sda6</code> と <code
                      class="filename">sdc6</code> パーティションは、それぞれ約 100 GB の 2 つの新しい RAID-1 ボリューム <code
                      class="filename">md1</code> と <code
                      class="filename">md2</code> に組み上げられます。これらのミラーは LVM の物理ボリュームとして初期化され、<code
                      class="filename">vg_raid</code> ボリュームグループに割り当てられます。<code
                      class="filename">vg_raid</code> VG は約 200 GB の安全な領域になります。
					</div></li><li
                  class="listitem"><div
                    class="para">
						残りのパーティションである <code
                      class="filename">sda7</code> と <code
                      class="filename">sdc7</code> は直接的に物理ボリュームとして使われ、<code
                      class="filename">vg_bulk</code> と呼ばれる別の VG に割り当てられます。これはおよそ 200 GB の領域になります。
					</div></li></ul></div><div
              class="para">
				VG を作成したら、VG をとても柔軟な方法でパーティショニングすることが可能です。<code
                class="filename">vg_raid</code> に作成された LV は 1 台にディスク障害に対して耐性を持ちますが、<code
                class="filename">vg_bulk</code> の場合そうではない点を忘れないでください。逆に、<code
                class="filename">vg_bulk</code> は両方のディスクにわたって割り当てられるので、巨大なファイルの読み書き速度が高速化されるでしょう。
			</div><div
              class="para">
				<code
                class="filename">lv_usr</code>、<code
                class="filename">lv_var</code>、<code
                class="filename">lv_home</code> という LV を <code
                class="filename">vg_raid</code> に作成し、それぞれに対応するファイルシステムをホストするようにします。一方、別の大きな LV <code
                class="filename">lv_movies</code> は編集済みの最終版の映像をホストするために使われます。<code
                class="filename">vg_bulk</code> を、デジタルビデオカメラから取り出したデータ用の大きな <code
                class="filename">lv_rushes</code> と、一時ファイル用の <code
                class="filename">lv_tmp</code>、に分割します。作業領域を作成する場所は簡単に決められるものではありません。つまり、作業領域用のボリュームは良い性能を必要としますが、編集作業中にディスク障害が起きた場合に作業内容を保護する必要があるでしょうか? この質問の回答次第で、対応する LV を <code
                class="filename">vg_raid</code> か <code
                class="filename">vg_bulk</code> のどちらの VG に作成するかが決まります。
			</div><div
              class="para">
				これで、重要なデータ用に多少の冗長性と、利用できる領域が用途ごとにどのように分割されるかに関する大きな柔軟性が確保されました。後から (たとえば音声クリップの編集用に) 新しいソフトウェアをインストールする場合も、<code
                class="filename">/usr/</code> をホストしている LV を簡単に増加することが可能です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> なぜ 3 種類の RAID-1 ボリュームが必要なのでしょうか?</strong></p></div></div></div><div
                class="para">
				<code
                  class="filename">vg_raid</code> 用の物理ボリュームとして提供される RAID-1 ボリュームを 1 つだけセットアップすることも可能でした。それにも関わらず、なぜ 3 種類の RAID-1 ボリュームを作成したのでしょうか?
			</div><div
                class="para">
				最初の分割 (<code
                  class="filename">md0</code> とその他) の根本的理由はデータの安全性を考慮したためです。つまり RAID-1 ミラーの両方の要素に書き込まれるデータは全く同じだからです。そのため RAID 層を迂回し、片方のディスクだけを直接マウントすることが可能です。たとえば、カーネルバグや LVM メタ情報が破壊された場合、RAID と LVM ボリュームに含まれるディスクの配置などの重要なデータにアクセスするために最小限のシステムを起動することが可能です。そして、このメタ情報を再構成したり、ファイルにアクセスすることが可能です。こうすることで、システムを正常状態に戻すことが可能です。
			</div><div
                class="para">
				2 番目の分割 (<code
                  class="filename">md1</code> と <code
                  class="filename">md2</code>) の根本的理由は明確というわけではありませんが、将来の不明確さを認めていることに関連します。最初動画編集用ワークステーションを組み上げた時、要求される正確なストレージサイズを完全な精度で知る必要はありません。それどころか、ストレージサイズは時間経過に従い増加するかもしれません。われわれの場合、ビデオ素材と編集済みビデオクリップ用に実際に要求されるストレージ領域を事前に知ることはできません。特定のクリップがとても大きな量の素材を必要とし、冗長性データ用の VG はまだ半分以上が未使用状態の場合、ここから一部の未使用領域を再利用することが可能です。われわれは物理ボリュームの 1 つを削除、たとえば <code
                  class="filename">vg_raid</code> から <code
                  class="filename">md2</code> を削除し、これを <code
                  class="filename">vg_bulk</code> に直接割り当てるか (予想される作業時間が一時的な性能の低下を許容できる程度に十分短い場合に限ります)、<code
                  class="filename">md2</code> の RAID セットアップをやり直してその構成要素である <code
                  class="filename">sda6</code> と <code
                  class="filename">sdc6</code> を VG に統合する (この場合 100 GB ではなく 200 GB の増加になります)、ことが可能です。さらに <code
                  class="filename">lv_rushes</code> 論理ボリュームは必要に応じて増加させることが可能です。
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.ldap-directory.html"><strong>戻る</strong>11.7. LDAP ディレクトリ</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>上に戻る</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>ホーム</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>次へ</strong>12.2. 仮想化</a></li></ul><div
        id="translated_pages"><ul><li><a
              href="../ar-MA/advanced-administration.html">ar-MA</a></li><li><a
              href="../da-DK/advanced-administration.html">da-DK</a></li><li><a
              href="../de-DE/advanced-administration.html">de-DE</a></li><li><a
              href="../el-GR/advanced-administration.html">el-GR</a></li><li><a
              href="../en-US/advanced-administration.html">en-US</a></li><li><a
              href="../es-ES/advanced-administration.html">es-ES</a></li><li><a
              href="../fa-IR/advanced-administration.html">fa-IR</a></li><li><a
              href="../fr-FR/advanced-administration.html">fr-FR</a></li><li><a
              href="../hr-HR/advanced-administration.html">hr-HR</a></li><li><a
              href="../id-ID/advanced-administration.html">id-ID</a></li><li><a
              href="../it-IT/advanced-administration.html">it-IT</a></li><li><a
              href="../ja-JP/advanced-administration.html">ja-JP</a></li><li><a
              href="../pl-PL/advanced-administration.html">pl-PL</a></li><li><a
              href="../pt-BR/advanced-administration.html">pt-BR</a></li><li><a
              href="../ro-RO/advanced-administration.html">ro-RO</a></li><li><a
              href="../ru-RU/advanced-administration.html">ru-RU</a></li><li><a
              href="../tr-TR/advanced-administration.html">tr-TR</a></li><li><a
              href="../zh-CN/advanced-administration.html">zh-CN</a></li></ul></div></body></html>
