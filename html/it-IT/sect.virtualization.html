<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. Virtualizzazione</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-it-IT-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preimpostazione, Monitoraggio, Virtualizzazione, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Il manuale dell'amministratore Debian" /><link
        rel="up"
        href="advanced-administration.html"
        title="Capitolo 12. Amministrazione avanzata" /><link
        rel="prev"
        href="advanced-administration.html"
        title="Capitolo 12. Amministrazione avanzata" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. Installazione automatica" /><link
        rel="canonical"
        href="http://l.github.io/debian-handbook/html/it-IT/sect.virtualization.html" /></head><body
      class="draft "><noscript><iframe
          src="//www.googletagmanager.com/ns.html?id=GTM-5H35QX"
          height="0"
          width="0"
          style="display:none;visibility:hidden"></iframe></noscript><script
        type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5H35QX');</script><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="http://debian-handbook.info"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Indietro</strong></a></li><li
          class="home">Il manuale dell'amministratore Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Avanti</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  id="sect.virtualization"></a>12.2. Virtualizzazione</h2></div></div></div><a
          id="idm140109879576144"
          class="indexterm"></a><div
          class="para">
			Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security.
		</div><div
          class="para">
			Ci sono numerose soluzioni di virtualizzazione, ciascuna coi suoi pro e contro. Questo libro si concentrerà su Xen, LXC e KVM, ma fra le altre implementazioni degne di nota vi sono le seguenti:
		</div><a
          id="idm140109879573520"
          class="indexterm"></a><a
          id="idm140109879572400"
          class="indexterm"></a><a
          id="idm140109879571280"
          class="indexterm"></a><a
          id="idm140109879570160"
          class="indexterm"></a><a
          id="idm140109879569040"
          class="indexterm"></a><a
          id="idm140109879567920"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <span
                  class="emphasis"><em>amd64</em></span> system can emulate an <span
                  class="emphasis"><em>arm</em></span> computer. QEMU is free software. <div
                  class="url">→ <a
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <div
                  class="url">→ <a
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <div
                  class="url">→ <a
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="idm140109880116048"
              class="indexterm"></a> è una situazione di «paravirtualizzazione». Introduce un sottile strato di astrazione, chiamato «ipervisore», fra l'hardware e i sistemi superiori; ciò agisce come un arbitro che controlla l'accesso all'hardware dalle macchine virtuali. Tuttavia, questo gestisce solo alcune delle istruzioni, mentre il resto è eseguito direttamente dall'hardware per conto dei sistemi. Il vantaggio principale è che non c'è degrado di prestazioni e i sistemi girano a una velocità prossima a quella nativa; il difetto è che i kernel dei sistemi operativi che si vogliono usare su un ipervisore Xen devono essere adattati per girare su Xen.
			</div><div
            class="para">
				Un po' di terminologia. L'ipervisore è lo strato inferiore, che gira direttamente sull'hardware, addirittura sotto il kernel. Questo ipervisore può dividere il resto del software su più <span
              class="emphasis"><em>domini</em></span>, che possono essere visti come altrettante macchine virtuali. Uno di questi domini (il primo che viene avviato) si chiama <span
              class="emphasis"><em>dom0</em></span> e ha un ruolo speciale, in quanto solo questo dominio può controllare l'ipervisore e l'esecuzione di altri domini. Questi altri domini si chiamano <span
              class="emphasis"><em>domU</em></span>. In altre parole, e dal punto di vista dell'utente, il <span
              class="emphasis"><em>dom0</em></span> coincide con l'«host» di altri sistemi di virtualizzazione, mentre un <span
              class="emphasis"><em>domU</em></span> può essere visto come «guest».
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURA</em></span> Xen e le varie versioni di Linux</strong></p></div></div></div><div
              class="para">
				Xen inizialmente fu sviluppato come un insieme di patch al di fuori dell'albero ufficiale e non integrate nel kernel Linux. Allo stesso tempo, diversi nuovi sistemi di virtualizzazione (incluso KVM) richiedevano alcune funzioni generiche relative alla virtualizzazione per facilitare la loro integrazione e il kernel Linux incluse queste funzioni (note come interfaccia <span
                class="emphasis"><em>paravirt_ops</em></span> o <span
                class="emphasis"><em>pv_ops</em></span>). Dal momento che le patch Xen duplicavano alcune delle funzionalità di questa interfaccia, non potevano essere accettate ufficialmente.
			</div><div
              class="para">
				Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				Since <span
                class="distribution distribution">Jessies</span> is based on version 3.16 of the Linux kernel, the standard <span
                class="pkg pkg">linux-image-686-pae</span> and <span
                class="pkg pkg">linux-image-amd64</span> packages include the necessary code, and the distribution-specific patching that was required for <span
                class="distribution distribution">Squeeze</span> and earlier versions of Debian is no more. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTA</em></span> Architetture compatibili con Xen</strong></p></div></div></div><div
              class="para">
				Xen is currently only available for the i386, amd64, arm64 and armhf architectures.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURA</em></span> Xen e kernel non Linux</strong></p></div></div></div><div
              class="para">
				Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				Tuttavia, se Xen può basarsi sulle funzioni hardware dedicate alla virtualizzazione (che sono presenti solo nei processori più recenti), anche sistemi operativi non modificati possono girare come domU (compreso Windows).
			</div></div><div
            class="para">
				L'uso di Xen sotto Debian richiede tre componenti:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						The hypervisor itself. According to the available hardware, the appropriate package will be either <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>, or <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						L'architettura i386 richiede inoltre una libreria standard con le patch appropriate che si appoggino a Xen; questa si trova nel pacchetto <span
                    class="pkg pkg">libc6-xen</span>.
					</div></li></ul></div><div
            class="para">
				In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <span
              class="pkg pkg">xen-linux-system-amd64</span>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <span
              class="pkg pkg">xen-utils-4.4</span>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				Una volta installati questi prerequisiti, il passo successivo è collaudare il comportamento del dom0 da solo; questo richiede un riavvio per entrare nell'ipervisore e nel kernel Xen. Il sistema dovrebbe avviarsi nel modo consueto, mostrando alcuni messaggi in più nella console durante i primi passi dell'inizializzazione.
			</div><div
            class="para">
				Ora è il momento di installare veramente dei sistemi utili sui sistemi domU, usando gli strumenti di <span
              class="pkg pkg">xen-tools</span>. Questo pacchetto fornisce il comando <code
              class="command">xen-create-image</code>, che automatizza gran parte del compito. L'unico parametro obbligatorio è <code
              class="literal">--hostname</code>, che dà un nome al domU; altre opzioni sono importanti, ma possono essere memorizzate nel file di configurazione <code
              class="filename">/etc/xen-tools/xen-tools.conf</code> e la loro mancanza dalla riga di comando non genera un errore. Perciò è importante controllare i contenuti di questo file prima di creare delle immagini oppure, in alternativa, usare parametri aggiuntivi nell'esecuzione di <code
              class="command">xen-create-image</code>. I parametri importanti includono:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>, per specificare la quantità di RAM dedicata al sistema appena creato;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> e <code
                    class="literal">--swap</code>, per definire la dimensione dei «dischi virtuali» disponibili al domU;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>, to cause the new system to be installed with <code
                    class="command">debootstrap</code>; in that case, the <code
                    class="literal">--dist</code> option will also most often be used (with a distribution name such as <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>APPROFONDIMENTI</em></span> Installare un sistema non Debian in un domU</strong></p></div></div></div><div
                    class="para">
						In caso di un sistema non Linux, bisogna fare attenzione a definire il kernel che il domU deve usare, usando l'opzione <code
                      class="literal">--kernel</code>.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> dichiara che la configurazione di rete del domU deve essere ottenuta tramite DHCP mentre <code
                    class="literal">--ip</code> permette di definire un indirizzo IP statico.
					</div></li><li
                class="listitem"><div
                  class="para">
						Da ultimo, bisogna scegliere un metodo di memorizzazione per le immagini da creare (quelle che saranno viste come dischi fissi dal domU). Il metodo più semplice, che corrisponde all'opzione <code
                    class="literal">--dir</code>, è di creare un file sul dom0 per ogni dispositivo da rendere disponibile al domU. Per i sistemi che usano LVM, l'alternativa è usare l'opzione <code
                    class="literal">--lvm</code>, seguita dal nome di un gruppo di volume; quindi <code
                    class="command">xen-create-image</code> creerà un nuovo volume logico dentro quel gruppo e questo volume logico sarà reso disponibile al domU come disco fisso.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>NOTA</em></span> Memorizzazione nel domU</strong></p></div></div></div><div
                    class="para">
						Oltre a partizioni, array RAID o volumi logici LVM preesistenti, anche interi dischi fissi possono essere esportati verso il domU. Queste operazioni non sono tuttavia automatizzate da <code
                      class="command">xen-create-image</code>, quindi è necessario modificare il file di configurazione dell'immagine Xen dopo la sua creazione iniziale con <code
                      class="command">xen-create-image</code>.
					</div></div></li></ul></div><div
            class="para">
				Una volta effettuate queste scelte, si può creare l'immagine per il futuro domU Xen:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				Adesso è stata creata una macchina virtuale, ma attualmente non è in esecuzione (e quindi occupa solo spazio sul disco fisso del dom0). Ovviamente si possono creare altre immagini, magari con parametri diversi.
			</div><div
            class="para">
				Prima di accendere queste macchine virtuali, bisogna definirne le modalità di accesso. Ovviamente possono essere considerate come macchine isolate, a cui si accederà tramite la loro console di sistema, ma raramente vengono usate in questo modo. Nella maggior parte dei casi, un domU sarà considerato un server remoto e vi si accederà solo via rete. Tuttavia sarebbe molto scomodo aggiungere una scheda di rete per ogni domU; per questo Xen permette di creare interfacce virtuali, che ogni dominio può vedere e usare in modo standard. Notare che queste schede, seppur virtuali, saranno utili solo una volta connesse a una rete, anche solo virtuale. A questo scopo, Xen ha diversi modelli di rete:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Il modello più semplice è il modello <span
                    class="emphasis"><em>bridge</em></span>; tutte le schede di rete eth0 (sia nel dom0 che nei sistemi domU) si comportano come se fossero direttamente inserite in uno switch Ethernet.
					</div></li><li
                class="listitem"><div
                  class="para">
						C'è poi il modello <span
                    class="emphasis"><em>routing</em></span>, dove il dom0 si comporta come un router che sta fra i sistemi domU e la rete esterna (fisica).
					</div></li><li
                class="listitem"><div
                  class="para">
						Infine, nel modello <span
                    class="emphasis"><em>NAT</em></span>, il dom0 è di nuovo fra i sistemi domU e il resto della rete, ma i sistemi domU non sono direttamente accessibili dall'esterno e il traffico passa attraverso alcune traduzioni degli indirizzi di rete sul dom0.
					</div></li></ul></div><div
            class="para">
				Queste tre modalità di rete comprendono alcune interfacce dai nomi insoliti, come <code
              class="filename">vif*</code>, <code
              class="filename">veth*</code>, <code
              class="filename">peth*</code> e <code
              class="filename">xenbr0</code>. L'ipervisore Xen le dispone in qualunque configurazione sia stata definita, sotto il controllo degli strumenti nello spazio utente. Poiché le modalità NAT e routing si adattano solo a casi particolari, qui si descriverà solo il modello di bridge.
			</div><div
            class="para">
				The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <code
              class="command">xend</code> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <code
              class="filename">xenbr0</code> taking precedence if several such bridges exist). We must therefore set up a bridge in <code
              class="filename">/etc/network/interfaces</code> (which requires installing the <span
              class="pkg pkg">bridge-utils</span> package, which is why the <span
              class="pkg pkg">xen-utils-4.4</span> package recommends it) to replace the existing eth0 entry:
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <code
              class="command">xl</code> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> Choice of toolstacks to manage Xen VM</strong></p></div></div></div><a
              id="idm140109880039424"
              class="indexterm"></a><a
              id="idm140109880038304"
              class="indexterm"></a><div
              class="para">
				In Debian 7 and older releases, <code
                class="command">xm</code> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <code
                class="command">xl</code> which is mostly backwards compatible. But those are not the only available tools: <code
                class="command">virsh</code> of libvirt and <code
                class="command">xe</code> of XenServer's XAPI (commercial offering of Xen) are alternative tools.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>ATTENZIONE</em></span> Solo un domU per immagine!</strong></p></div></div></div><div
              class="para">
				Anche se è ovviamente possibile far girare più sistemi domU in parallelo, ognuno di essi deve usare la propria immagine, dal momento che ogni domU crede di girare sul proprio hardware (a parte la piccola parte del kernel che parla all'ipervisore). In particolare, non è possibile che due sistemi domU girino simultaneamente sullo stesso spazio disco. Se tuttavia i sistemi domU non sono contemporaneamente in esecuzione, è del tutto possibile riutilizzare una singola partizione di swap o la partizione che ospita il file system <code
                class="filename">/home</code>.
			</div></div><div
            class="para">
				Notare che il domU <code
              class="filename">testxen</code> usa memoria fisica presa dalla RAM che altrimenti sarebbe disponibile per il dom0, non memoria simulata. Pertanto bisogna fare attenzione, quando si assembla un server che deve ospitare istanze di Xen, a fornire RAM fisica secondo le necessità.
			</div><div
            class="para">
				Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <code
              class="filename">hvc0</code> console, with the <code
              class="command">xl console</code> command:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				A questo punto si può aprire una sessione, proprio come si farebbe davanti alla tastiera della macchina virtuale. Lo scollegamento da questa console si ottiene con la combinazione di tasti <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>SUGGERIMENTO</em></span> Arrivare subito alla console</strong></p></div></div></div><div
              class="para">
				Sometimes one wishes to start a domU system and get to its console straight away; this is why the <code
                class="command">xl create</code> command takes a <code
                class="literal">-c</code> switch. Starting a domU with this switch will display all the messages as the system boots.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (in the <span
                class="pkg pkg">openxenmanager</span> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <code
                class="command">xl</code> command.
			</div></div><div
            class="para">
				Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <code
              class="command">xl pause</code> and <code
              class="command">xl unpause</code> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <code
              class="command">xl save</code> and <code
              class="command">xl restore</code> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOCUMENTATION</em></span> <code
                        class="command">xl</code> options</strong></p></div></div></div><div
              class="para">
				Most of the <code
                class="command">xl</code> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span> manual page.
			</div></div><div
            class="para">
				Halting or rebooting a domU can be done either from within the domU (with the <code
              class="command">shutdown</code> command) or from the dom0, with <code
              class="command">xl shutdown</code> or <code
              class="command">xl reboot</code>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>APPROFONDIMENTI</em></span> Xen avanzato</strong></p></div></div></div><div
              class="para">
				Xen ha molte più funzionalità di quanto si possa descrivere in queste poche righe. In particolare, il sistema è molto dinamico e molti parametri di un dominio (come la quantità di memoria allocata, i dischi fissi visibili, il comportamento del task scheduler e così via) possono essere variati anche quando quel dominio è in esecuzione. Un domU può anche essere migrato su un altro server senza venire spento e senza perdere le sue connessioni di rete. Per tutti questi aspetti avanzati, la fonte primaria di informazioni è la documentazione ufficiale di Xen. <div
                class="url">→ <a
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="idm140109880005648"
            class="indexterm"></a><div
            class="para">
				Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <span
              class="emphasis"><em>control groups</em></span>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system.
			</div><div
            class="para">
				These features can be combined to isolate a whole process family starting from the <code
              class="command">init</code> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <span
              class="emphasis"><em>LinuX Containers</em></span>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTA</em></span> Limiti di isolamento di LXC</strong></p></div></div></div><div
              class="para">
				I contenitori LXC non forniscono il livello di isolamento raggiunto da emulatori o virtualizzatori più pesanti. In particolare:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						the <span
                      class="distribution distribution">Jessie</span> standard kernel does not allow limiting the amount of memory available to a container; the feature exists, and is built in the kernel, but it is disabled by default because it has a (slight) cost on overall system performance; however, enabling it is a simple matter of setting the <code
                      class="command">cgroup_enable=memory</code> kernel command-line option at boot time;
					</div></li><li
                  class="listitem"><div
                    class="para">
						poiché il kernel è condiviso fra il sistema host e i contenitori, i processi limitati ai contenitori possono ancora accedere ai messaggi del kernel, il che può portare a fughe di informazioni se i messaggi sono emessi da un contenitore;
					</div></li><li
                  class="listitem"><div
                    class="para">
						per ragioni simili, se un contenitore è compromesso e viene sfruttata una vulnerabilità del kernel, gli altri contenitori possono anch'essi esserne affetti;
					</div></li><li
                  class="listitem"><div
                    class="para">
						sul file system, il kernel controlla i permessi in base agli identificativi numerici per utenti e gruppi; questi identificativi possono designare utenti e gruppi diversi a seconda del contenitore, cosa di cui tener conto se si condividono parti scrivibili del file system fra i contenitori.
					</div></li></ul></div></div><div
            class="para">
				Poiché si parla di isolamento e non di virtualizzazione vera e propria, impostare i contenitori LXC è più complesso che far girare debian-installer su una macchina virtuale. Verranno descritti alcuni prerequisiti e poi si passerà alla configurazione di rete; a questo punto si potrà effettivamente creare il sistema da far girare nel contenitore.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879991344"></a>12.2.2.1. Passi preliminari</h4></div></div></div><div
              class="para">
					Il pacchetto <span
                class="pkg pkg">lxc</span> contiene gli strumenti necessari per far girare LXC e quindi deve essere installato.
				</div><div
              class="para">
					LXC also requires the <span
                class="emphasis"><em>control groups</em></span> configuration system, which is a virtual filesystem to be mounted on <code
                class="filename">/sys/fs/cgroup</code>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="sect.lxc.network"></a>12.2.2.2. Configurazione di rete</h4></div></div></div><div
              class="para">
					Lo scopo dell'installazione di LXC è di impostare delle macchine virtuali; pur potendo ovviamente tenerle isolate dalla rete e comunicare con loro solo tramite il file system, la maggior parte dei casi d'uso richiede di dare almeno un minimo accesso di rete ai contenitori. Nel caso tipico, ciascun contenitore avrà un'interfaccia di rete virtuale, connessa con la rete reale tramite un bridge. Questa interfaccia virtuale può essere inserita direttamente sull'interfaccia fisica di rete dell'host (nel qual caso il contenitore è direttamente in rete) o su un'altra interfaccia virtuale definita sull'host (e l'host può allora filtrare o ridirigere il traffico). In entrambi i casi, sarà richiesto il pacchetto <span
                class="pkg pkg">bridge-utils</span>.
				</div><div
              class="para">
					Il caso semplice richiede solo di modificare <code
                class="filename">/etc/network/interfaces</code>, spostare la configurazione dell'interfaccia fisica (per esempio <code
                class="literal">eth0</code>) su un'interfaccia bridge (di solito <code
                class="literal">br0</code>) e configurare il link fra essi. Per esempio, se il file di configurazione dell'interfaccia di rete contiene voci come le seguenti:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					Devono essere disabilitate e sostituite con le seguenti:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					L'effetto di questa configurazione sarà simile a ciò che si otterrebbe se i contenitori fossero macchine collegate alla stessa rete fisica dell'host. La configurazione «bridge» gestisce il transito dei frame Ethernet fra tutte le interfacce in bridge, il che include la <code
                class="literal">eth0</code> fisica oltre alle interfacce definite per i contenitori.
				</div><div
              class="para">
					Nei casi in cui questa configurazione non si può usare (per esempio se non si possono assegnare IP pubblici ai contenitori), un'interfaccia virtuale <span
                class="emphasis"><em>tap</em></span> verrà creata e connessa al bridge. A quel punto la topologia di rete equivalente diventa quella di un host con una seconda scheda di rete inserita in uno switch separato, con i contenitori anch'essi inseriti in quello switch. L'host allora deve agire da gateway per i contenitori se questi devono comunicare con il mondo esterno.
				</div><div
              class="para">
					Oltre a <span
                class="pkg pkg">bridge-utils</span>, questa configurazione «ricca» richiede il pacchetto <span
                class="pkg pkg">vde2</span>; il file <code
                class="filename">/etc/network/interfaces</code> allora diventa:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre><div
              class="para">
					La rete allora può essere impostata staticamente nei contenitori o dinamicamente con un server DHCP che gira sull'host. Tale server DHCP dovrà essere configurato per rispondere alle richieste sull'interfaccia <code
                class="literal">br0</code>.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879973520"></a>12.2.2.3. Impostazione del sistema</h4></div></div></div><div
              class="para">
					Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <span
                class="pkg pkg">lxc</span> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <span
                class="pkg pkg">debootstrap</span> and <span
                class="pkg pkg">rsync</span> packages) will install a Debian container:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					Notare che il file system è creato all'inizio in <code
                class="filename">/var/cache/lxc</code> e poi spostato nella sua directory di destinazione. Ciò permette di creare contenitori identici molto più rapidamente, visto che a questo punto basta copiarli.
				</div><div
              class="para">
					Note that the debian template creation script accepts an <code
                class="option">--arch</code> option to specify the architecture of the system to be installed and a <code
                class="option">--release</code> option if you want to install something else than the current stable release of Debian. You can also set the <code
                class="literal">MIRROR</code> environment variable to point to a local Debian mirror.
				</div><div
              class="para">
					The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) and add a few <code
                class="literal">lxc.network.*</code> entries:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</pre><div
              class="para">
					Queste voci vogliono dire, rispettivamente, che verrà creata un'interfaccia virtuale nel contenitore; che verrà automaticamente attivata quando il suddetto contenitore verrà avviato; che verrà automaticamente connessa al bridge <code
                class="literal">br0</code> sull'host; e che il suo indirizzo MAC sarà quello specificato. Se l'ultima voce fosse assente o disabilitata, verrà generato un indirizzo MAC casuale.
				</div><div
              class="para">
					Another useful entry in that file is the setting of the hostname:
				</div><pre
              class="programlisting">lxc.utsname = testlxc
</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879960000"></a>12.2.2.4. Avvio del contenitore</h4></div></div></div><div
              class="para">
					Ora che l'immagine della macchina virtuale è pronta, si avvia il contenitore:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>). We can exit the console with <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span>.
				</div><div
              class="para">
					Note that we ran the container as a background process, thanks to the <code
                class="option">--daemon</code> option of <code
                class="command">lxc-start</code>. We can interrupt the container with a command such as <code
                class="command">lxc-stop --name=testlxc</code>.
				</div><div
              class="para">
					The <span
                class="pkg pkg">lxc</span> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <code
                class="command">lxc-autostart</code> which starts containers whose <code
                class="literal">lxc.start.auto</code> option is set to 1). Finer-grained control of the startup order is possible with <code
                class="literal">lxc.start.order</code> and <code
                class="literal">lxc.group</code>: by default, the initialization script first starts containers which are part of the <code
                class="literal">onboot</code> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <code
                class="literal">lxc.start.order</code> option.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>APPROFONDIMENTI</em></span> Virtualizzazione di massa</strong></p></div></div></div><div
                class="para">
					Poiché LXC è un sistema di isolamento molto leggero, si può adattare in particolare all'hosting massiccio di server virtuali. La configurazione di rete probabilmente sarà un po' più avanzata di quella descritta sopra, ma la configurazione «ricca» che usa le interfacce <code
                  class="literal">tap</code> e <code
                  class="literal">veth</code> dovrebbe bastare in molti casi.
				</div><div
                class="para">
					Può anche avere senso condividere parte del file system, come i sottoalberi <code
                  class="filename">/usr</code> e <code
                  class="filename">/lib</code>, così da evitare di duplicare software che dovrebbe essere in comune a diversi contenitori. Questo di solito si può fare aggiungendo delle voci <code
                  class="literal">lxc.mount.entry</code> nei file di configurazione dei contenitori. Un effetto collaterale interessante è che in questo caso i processi useranno meno memoria fisica, dal momento che il kernel può rilevare che i programmi sono condivisi. Il costo marginale di un contenitore in più può quindi essere ridotto allo spazio su disco dedicato ai suoi dati specifici e alcuni processi aggiuntivi che il kernel deve ordinare e gestire.
				</div><div
                class="para">
					We haven't described all the available options, of course; more comprehensive information can be obtained from the <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> and <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span> manual pages and the ones they reference.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="idm140109879934704"></a>12.2.3. Virtualizzazione con KVM</h3></div></div></div><a
            id="idm140109879933936"
            class="indexterm"></a><div
            class="para">
				KVM, which stands for <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <code
              class="command">qemu-*</code> commands: it is still about KVM.
			</div><div
            class="para">
				Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <code
              class="filename">/proc/cpuinfo</code>.
			</div><div
            class="para">
				With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879929296"></a>12.2.3.1. Passi preliminari</h4></div></div></div><a
              id="idm140109879928496"
              class="indexterm"></a><div
              class="para">
					Contrariamente a strumenti come VirtualBox, KVM di per sé non include un'interfaccia utente per creare e gestire macchine virtuali. Il pacchetto <span
                class="pkg pkg">qemu-kvm</span> fornisce solo un eseguibile in grado di avviare una macchina virtuale, oltre a uno script di inizializzazione che carica i moduli appropriati del kernel.
				</div><a
              id="idm140109879925824"
              class="indexterm"></a><a
              id="idm140109879924864"
              class="indexterm"></a><div
              class="para">
					Per fortuna, Red Hat fornisce anche un altro insieme di strumenti per affrontare questo problema, sviluppando la libreria <span
                class="emphasis"><em>libvirt</em></span> e gli strumenti <span
                class="emphasis"><em>virtual machine manager</em></span> associati. libvirt permette di gestire macchine virtuali in modo uniforme, indipendentemente dal sistema di virtualizzazione dietro le quinte (attualmente supporta QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare e UML). <code
                class="command">virtual-manager</code> è un'interfaccia grafica che usa libvirt per creare e gestire macchine virtuali.
				</div><a
              id="idm140109879921232"
              class="indexterm"></a><div
              class="para">
					Prima di tutto si installano i pacchetti richiesti, con <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code>. <span
                class="pkg pkg">libvirt-bin</span> fornisce il demone <code
                class="command">libvirtd</code>, che permette di gestire (potenzialmente da remoto) le macchine virtuali che girano sull'host e fa partire le VM richieste all'avvio dell'host. Inoltre, questo pacchetto fornisce lo strumento a riga di comando <code
                class="command">virsh</code>, che permette di controllare le macchine gestite da <code
                class="command">libvirtd</code>.
				</div><div
              class="para">
					Il pacchetto <span
                class="pkg pkg">virtinst</span> fornisce <code
                class="command">virt-install</code>, che permette di creare macchine virtuali da riga di comando. Infine, <span
                class="pkg pkg">virt-viewer</span> permette di accedere alla console grafica di una VM.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879913584"></a>12.2.3.2. Configurazione di rete</h4></div></div></div><div
              class="para">
					Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">Sezione 12.2.2.2, «Configurazione di rete»</a>).
				</div><div
              class="para">
					In alternativa e nella configurazione predefinita fornita da KVM, alla macchina virtuale è assegnato un indirizzo privato (nell'intervallo 192.168.122.0/24) e viene impostato il NAT cosicché la VM possa accedere alla rete esterna.
				</div><div
              class="para">
					Il resto di questa sezione assume che l'host abbia un'interfaccia fisica <code
                class="literal">eth0</code> e un bridge <code
                class="literal">br0</code> e che la prima sia connessa al secondo.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109879909312"></a>12.2.3.3. Installazione con <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="idm140109879908192"
              class="indexterm"></a><div
              class="para">
					Creare una macchina virtuale è molto simile a installare un sistema normale, tranne che le caratteristiche della macchina virtuale sono descritte da una riga di comando che sembra infinita.
				</div><div
              class="para">
					Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">Sezione 9.2.2, «Utilizzo di desktop remoti grafici»</a> for details), which will allow us to control the installation process.
				</div><div
              class="para">
					Prima bisogna dire a libvirtd dove memorizzare le immagini su disco, se non va bene la posizione predefinita (<code
                class="filename">/var/lib/libvirt/images/</code>).
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIP</em></span> Add your user to the libvirt group</strong></p></div></div></div><div
                class="para">
					All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <code
                  class="literal">libvirt</code> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <code
                  class="literal">libvirt</code> group and run the various commands under your user identity.
				</div></div><div
              class="para">
					Si avvia il processo di installazione per la macchina virtuale e si guardano più da vicino le opzioni più importanti di <code
                class="command">virt-install</code>. Questo comando registra la macchina virtuale e i suoi parametri in libvirtd, quindi la avvia cosicché la sua installazione può procedere.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							L'opzione <code
                        class="literal">--connect</code> specifica l'«ipervisore» da usare. La sua forma è quella di un URL contenente un sistema di virtualizzazione (<code
                        class="literal">xen://</code>, <code
                        class="literal">qemu://</code>, <code
                        class="literal">lxc://</code>, <code
                        class="literal">openvz://</code>, <code
                        class="literal">vbox://</code> e così via) e la macchina che deve ospitare la VM (questo può essere lasciato vuoto nel caso dell'host locale). Inoltre e nel caso di QEMU/KVM ciascun utente può gestire macchine virtuali che funzionano con permessi ristretti e il percorso nell'URL permette di differenziare le macchine «di sistema» (<code
                        class="literal">/system</code>) dalle altre (<code
                        class="literal">/session</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Poiché KVM è gestito allo stesso modo di QEMU, <code
                        class="literal">--virt-type kvm</code> permette di specificare l'uso di KVM anche se l'URL sembra quello di QEMU.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							L'opzione <code
                        class="literal">--name</code> definisce un nome (unico) per la macchina virtuale.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							L'opzione <code
                        class="literal">--ram</code> permette di specificare la quantità di RAM (in MB) da allocare per la macchina virtuale.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--disk</code> specifica la posizione del file immagine che deve rappresentare il disco fisso della macchina virtuale; quel file è creato, se non presente, con una dimensione (in GB) specificata dal parametro <code
                        class="literal">size</code>. Il parametro <code
                        class="literal">format</code> permette di scegliere fra vari modi di memorizzare il file immagine. Il formato predefinito (<code
                        class="literal">raw</code>) è un singolo file che combacia esattamente con la dimensione e i contenuti del disco. Qui la scelta è di prendere un formato più avanzato, specifico di QEMU e che permette di iniziare con un file piccolo che cresce solo quando la macchina virtuale comincia effettivamente ad usare spazio.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							L'opzione <code
                        class="literal">--cdrom</code> è usata per indicare dove trovare il disco ottico da usare per l'installazione. Il percorso può essere un percorso locale di un file ISO, un URL dove reperire il file o il device di un lettore CD-ROM fisico (es. <code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--network</code> specifica come la scheda di rete virtuale si integra nella configurazione di rete dell'host. Il comportamento predefinito (che in questo esempio è esplicitamente forzato) è di integrarla in un qualunque bridge di rete preesistente. Se non esiste un tale bridge, la macchina virtuale raggiungerà la rete fisica solo tramite NAT, quindi riceve un indirizzo in un intervallo di una sottorete privata (192.168.122.0/24).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">Sezione 9.2.1.3, «Creazione di tunnel cifrati con il port forwarding»</a>). Alternatively, the <code
                        class="literal">--vnclisten=0.0.0.0</code> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Le opzioni <code
                        class="literal">--os-type</code> e <code
                        class="literal">--os-variant</code> permettono di ottimizzare alcuni parametri della macchina virtuale, basandosi su alcune delle funzionalità note del sistema operativo lì menzionato.
						</div></td></tr></table></div><div
              class="para">
					A questo punto la macchina virtuale è in esecuzione e bisogna connettersi alla console grafica per procedere con il processo di installazione. Se la precedente operazione è stata lanciata da un ambiente desktop grafico, questa connessione dovrebbe essere avviata automaticamente. In caso contrario, o in caso si operi da remoto, si può eseguire <code
                class="command">virt-viewer</code> da qualunque ambiente grafico per aprire la console grafica (notare che la password di root dell'host remoto viene chiesta due volte perché l'operazione richiede 2 connessioni SSH):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					Al termine del processo di installazione, la macchina virtuale viene riavviata ed è ora pronta all'uso.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109878546144"></a>12.2.3.4. Gestire macchine con <code
                      class="command">virsh</code></h4></div></div></div><a
              id="idm140109878545248"
              class="indexterm"></a><div
              class="para">
					Ora che l'installazione è terminata, si passa a come gestire le macchine virtuali disponibili. La prima cosa da provare è chiedere a <code
                class="command">libvirtd</code> la lista delle macchine virtuali che gestisce:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					Si avvia la macchina virtuale di prova:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					Si possono ora ottenere le istruzioni per connettersi alla console grafica (il display VNC restituito può essere passato come parametro a <code
                class="command">vncviewer</code>):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					Altri sottocomandi disponibili di <code
                class="command">virsh</code> includono:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> per riavviare una macchina virtuale;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> per provocare uno spegnimento pulito;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code> per fermarla brutalmente;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> per metterla in pausa;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> per farla uscire dalla pausa;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> per abilitare (o disabilitare, con l'opzione <code
                      class="literal">--disable</code>) l'avvio automatico della macchina virtuale all'avvio dell'host;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> per rimuovere ogni traccia della macchina virtuale da <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					Tutti questi sottocomandi accettano un identificatore di macchina virtuale come parametro.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="idm140109878528544"></a>12.2.3.5. Installing an RPM based system in Debian with yum</h4></div></div></div><div
              class="para">
					If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <code
                class="command">debootstrap</code>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <code
                class="command">yum</code> utility (available in the package of the same name).
				</div><div
              class="para">
					The procedure requires using <code
                class="command">rpm</code> to extract an initial set of files, including notably <code
                class="command">yum</code> configuration files. And then calling <code
                class="command">yum</code> to extract the remaining set of packages. But since we call <code
                class="command">yum</code> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen">
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"</code></strong>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm</code></strong>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath</code></strong>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm</code></strong>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm</code></strong>
<code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
</code>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo</code></strong>
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core</code></strong>
[...]
<code
                class="computeroutput"># </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo</code></strong>
</pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Indietro</strong>Capitolo 12. Amministrazione avanzata</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Risali</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Partenza</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Avanti</strong>12.3. Installazione automatica</a></li></ul><div
        id="translated_pages"><ul><li><a
              href="../ar-MA/sect.virtualization.html">ar-MA</a></li><li><a
              href="../da-DK/sect.virtualization.html">da-DK</a></li><li><a
              href="../de-DE/sect.virtualization.html">de-DE</a></li><li><a
              href="../el-GR/sect.virtualization.html">el-GR</a></li><li><a
              href="../en-US/sect.virtualization.html">en-US</a></li><li><a
              href="../es-ES/sect.virtualization.html">es-ES</a></li><li><a
              href="../fa-IR/sect.virtualization.html">fa-IR</a></li><li><a
              href="../fr-FR/sect.virtualization.html">fr-FR</a></li><li><a
              href="../hr-HR/sect.virtualization.html">hr-HR</a></li><li><a
              href="../id-ID/sect.virtualization.html">id-ID</a></li><li><a
              href="../it-IT/sect.virtualization.html">it-IT</a></li><li><a
              href="../ja-JP/sect.virtualization.html">ja-JP</a></li><li><a
              href="../pl-PL/sect.virtualization.html">pl-PL</a></li><li><a
              href="../pt-BR/sect.virtualization.html">pt-BR</a></li><li><a
              href="../ro-RO/sect.virtualization.html">ro-RO</a></li><li><a
              href="../ru-RU/sect.virtualization.html">ru-RU</a></li><li><a
              href="../tr-TR/sect.virtualization.html">tr-TR</a></li><li><a
              href="../zh-CN/sect.virtualization.html">zh-CN</a></li></ul></div></body></html>
